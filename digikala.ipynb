{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'اصلا خوب نبود قطعاتش داخله سوراخایی که بود جا نمیشد سوراخا خیلی کوچیک بودن و میتونم بگم که با زور جا انداختمش اصلا خوب نبود،،بنظرم ریلی بود کشوش خیلی بهتر بود چون الان کجو کلست،،حالا در کل منکه گرفتم،اما پیشنهادش نمیکنم.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xls = pd.ExcelFile('data.xls')\n",
    "df1 = pd.read_excel(xls, 'Export')\n",
    "df2 = pd.read_excel(xls, 'Sheet 2')\n",
    "data = pd.concat([df1, df2])\n",
    "data = data.reset_index(drop=True)\n",
    "data.loc[1]['comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'اصلا خوب نبود قطعاتش داخله سوراخایی که بود جا نمیشد سوراخا خیلی کوچیک بودن و میتونم بگم که با زور جا انداختمش اصلا خوب نبود،، بنظرم ریلی بود کشوش خیلی بهتر بود چون الان کجو کلست،، حالا در کل منکه گرفتم، اما پیشنهادش نمیکنم.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hazm import *\n",
    "normalizer = Normalizer()\n",
    "# Select Verified Comments\n",
    "data = data[data['verification_status'] == 'verified']\n",
    "\n",
    "# Combine Features ('title', 'comment', 'advantages', 'disadvantages')\n",
    "data['combined'] = data['title']+'. '+data['comment']+'. '+data['advantages']+'. '+data['disadvantages']\n",
    "\n",
    "# Remove other columns except this ones\n",
    "data = data[['combined', 'label', 'title', 'comment', 'advantages', 'disadvantages']]\n",
    "\n",
    "# Normalize each column\n",
    "data['comment'] = data['comment'].apply(normalizer.normalize)\n",
    "data['title'] = data['title'].apply(normalizer.normalize)\n",
    "data['advantages'] = data['advantages'].apply(normalizer.normalize)\n",
    "data['disadvantages'] = data['disadvantages'].apply(normalizer.normalize)\n",
    "data['combined'] = data['combined'].apply(normalizer.normalize)\n",
    "\n",
    "data.loc[1]['comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import hazm as hz\n",
    "\n",
    "def remove_mentions(input_text):\n",
    "    return re.sub(r'@\\w+', '', input_text)\n",
    "def remove_urls(input_text):\n",
    "    return re.sub(r'http.?://[^\\s]+[\\s]?', '', input_text)\n",
    "def emoji_oneword(input_text):\n",
    "    return input_text.replace('_','')\n",
    "def remove_punctuation(input_text):\n",
    "    punct = string.punctuation + '«»'\n",
    "    trantab = str.maketrans(punct, len(punct)*' ')\n",
    "    return input_text.translate(trantab)\n",
    "def remove_digits(input_text):\n",
    "    return re.sub('\\d+', '', input_text)\n",
    "def remove_english(input_text):\n",
    "#     return input_text\n",
    "    return re.sub('[a-zA-Z0-9?><;,{}[\\]\\-_+=!@#$%\\^&*|\\']*' ,'', input_text)\n",
    "def remove_stopwords(input_text):\n",
    "    return input_text\n",
    "#     stopwords_list = hz.stopwords_list()\n",
    "#     words = input_text.split() \n",
    "#     clean_words = [word for word in words if (word not in stopwords_list) and len(word) > 1] \n",
    "#     return \" \".join(clean_words) \n",
    "def stemming(input_text):\n",
    "    stemmer = Stemmer()\n",
    "    words = input_text.split() \n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return \" \".join(stemmed_words)\n",
    "def transform(data):\n",
    "    clean_data = data.apply(remove_mentions).apply(remove_urls).apply(emoji_oneword).apply(remove_punctuation).apply(remove_digits).apply(remove_english).apply(remove_stopwords).apply(stemming)\n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "combined         پس میده متاسفانه من چون همیشه مولفیکس استفاده ...\n",
       "label                                                            2\n",
       "title                                             پس میده متاسفانه\n",
       "comment          من چون همیشه مولفیکس استفاده کردم و هیچوقت مشک...\n",
       "advantages                                                    None\n",
       "disadvantages                                                 None\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['combined'] =  transform(data.combined)\n",
    "data = data.dropna()\n",
    "data.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "combined         اصلا خوب نبود اصلا خوب نبود قطعات داخله سوراخا...\n",
       "label                                                            2\n",
       "title                                               اصلا خوب نبود.\n",
       "comment          اصلا خوب نبود قطعاتش داخله سوراخایی که بود جا ...\n",
       "advantages                                                    None\n",
       "disadvantages                                                 None\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df = 5,\n",
    "                             max_df = 0.8,\n",
    "                             sublinear_tf = True,\n",
    "                             use_idf = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data = data[['combined', 'label']]\n",
    "train, test = train_test_split(data, test_size=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors = vectorizer.fit_transform(train['combined'])\n",
    "test_vectors = vectorizer.transform(test['combined'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8057"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "# Perform classification with SVM, kernel=linear\n",
    "classifier_linear = svm.SVC(probability=True, kernel='linear', gamma='auto')\n",
    "t0 = time.time()\n",
    "classifier_linear.fit(train_vectors, train['label'])\n",
    "t1 = time.time()\n",
    "prediction_linear = classifier_linear.predict(test_vectors)\n",
    "t2 = time.time()\n",
    "time_linear_train = t1-t0\n",
    "time_linear_predict = t2-t1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive:  {'precision': 0.8198903313492993, 'recall': 0.9283591594141372, 'f1-score': 0.8707598118513725, 'support': 18844}\n",
      "negative:  {'precision': 0.7362494073020389, 'recall': 0.7704999379729562, 'f1-score': 0.7529853912832636, 'support': 8061}\n",
      "negative:  {'precision': 0.39568345323741005, 'recall': 0.018290655138011307, 'f1-score': 0.03496503496503497, 'support': 3007}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'0.0': {'precision': 0.7362494073020389,\n",
       "  'recall': 0.7704999379729562,\n",
       "  'f1-score': 0.7529853912832636,\n",
       "  'support': 8061},\n",
       " '1.0': {'precision': 0.39568345323741005,\n",
       "  'recall': 0.018290655138011307,\n",
       "  'f1-score': 0.03496503496503497,\n",
       "  'support': 3007},\n",
       " '2.0': {'precision': 0.8198903313492993,\n",
       "  'recall': 0.9283591594141372,\n",
       "  'f1-score': 0.8707598118513725,\n",
       "  'support': 18844},\n",
       " 'accuracy': 0.7943300347686547,\n",
       " 'macro avg': {'precision': 0.6506077306295828,\n",
       "  'recall': 0.5723832508417016,\n",
       "  'f1-score': 0.5529034126998904,\n",
       "  'support': 29912},\n",
       " 'weighted avg': {'precision': 0.7547051357345822,\n",
       "  'recall': 0.7943300347686547,\n",
       "  'f1-score': 0.7549997657729846,\n",
       "  'support': 29912}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# results\n",
    "\n",
    "report = classification_report(test['label'], prediction_linear, output_dict=True)\n",
    "print('positive: ', report['2.0'])\n",
    "print('negative: ', report['0.0'])\n",
    "print('neutral: ', report['1.0'])\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "اصلا خوب نبود بنظر میتونس خیل به باشه \n",
      "\n",
      "\n",
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "review = \"\"\"\n",
    "اصلا خوب نبود بنظرم میتونست خیلی بهتر باشه\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "review = pd.Series([review])\n",
    "review = transform(review)\n",
    "print(review.loc[0], \"\\n\\n\")\n",
    "review_vector = vectorizer.transform([review.loc[0]]) # vectorizing\n",
    "print(classifier_linear.predict(review_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "کاملا راض \n",
      "\n",
      "\n",
      "[2.]\n"
     ]
    }
   ],
   "source": [
    "review = \"\"\"\n",
    "کاملا راضیم\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "review = pd.Series([review])\n",
    "review = transform(review)\n",
    "print(review.loc[0], \"\\n\\n\")\n",
    "review_vector = vectorizer.transform([review.loc[0]]) # vectorizing\n",
    "print(classifier_linear.predict(review_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "این همون یا حت میشه گف هس که ارتقا دادن واقعا چ داره که مرد برن آیفون  قبل شون رو بدن اینو بگیرن با قیم نجوم اون تو سال که ب برند  ب حاشیه زدن حالا درسته که ب حاشیه هس ول اون اشکال خود رو داره مثل بیرون زدگ دوربین با وجود ضخام ب نسب به قبل و حذف سنسور اثر انگ حت سامسونگ ه با اون همه سنسور مختلف باز دوربین نزده بیرون یا به این اندازه ضخام نداره \n",
      "\n",
      "\n",
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "review = \"\"\"\n",
    "این همون iPhone 7 یا حتی میشه گفت iPhone 6s هست که ارتقا دادنش واقعا چی داره که مردم برن آیفون های قبلی شون رو بدن اینو بگیرن با قیمت نجومی اونم تو سال 2017 که بیشتر برند ها بی حاشیه زدن حالا درسته که iPhone x بی حاشیه هست ولی اونم اشکالات خودش رو داره مثل بیرون زدگی دوربین با وجود ضخامت بیشتر نسبت به قبل و حذف سنسور اثر انگشت حتی سامسونگ هم با اون همه سنسور مختلف باز دوربین نزده بیرون یا به این اندازه ضخامت نداره\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "review = pd.Series([review])\n",
    "review = transform(review)\n",
    "print(review.loc[0], \"\\n\\n\")\n",
    "review_vector = vectorizer.transform([review.loc[0]]) # vectorizing\n",
    "print(classifier_linear.predict(review_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['classifier_linear.sav']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib as jl\n",
    "\n",
    "filename = 'classifier_linear.sav'\n",
    "jl.dump(classifier_linear, filename)\n",
    "\n",
    "filename = 'prediction_linear.sav'\n",
    "jl.dump(prediction_linear, filename)\n",
    "\n",
    "filename = 'test.sav'\n",
    "jl.dump(test, filename)\n",
    "\n",
    "filename = 'test_label.sav'\n",
    "jl.dump(test['label'], filename)\n",
    "\n",
    "filename = 'train.sav'\n",
    "jl.dump(train, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
